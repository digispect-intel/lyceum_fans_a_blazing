# AI Inference Runtime & Power Estimation

Predict AI model inference runtime and power consumption across different hardware platforms.

## Team
- [Akshitha Chintaguntla Raghuram](https://github.com/akshithacr12)
- [Nahid Taherkhani](https://github.com/Nahid-taherkhani)
- [David McGrath](https://github.com/digispect-intel)
- [Yelyzaveta Bespalova](https://github.com/lizabespalova) 

## Challenge Overview
Build a system that estimates inference time and/or power usage of AI models on specific hardware platforms, bridging AI, compiler design, and systems engineering.

## ğŸ¯ Key Findings for Judges

**ğŸ† HACKATHON RESULTS: Comprehensive AI Hardware Performance Analysis**

Predict AI model inference runtime and power consumption across different hardware platforms, bridging AI, compiler design, and systems engineering.

### **ğŸ“Š Experimental Scale**
- **3,268 total experiments** conducted across comprehensive test matrix
- **27 hardware/model configurations** systematically evaluated
- **11 different AI models** tested (from 124M to 1.4B parameters)
- **2 independent data sources** validating results

### **ğŸš€ Performance Discovery**
- **Significant performance variations** across hardware configurations
- **GPU vs CPU analysis** showing clear acceleration benefits
- **Model size impact** on inference performance across different hardware
- **Systematic parameter sweeps** across batch sizes, prompt types, generation settings

### **ğŸ’¡ Business Impact**
- **Data-driven hardware selection** for optimal AI inference performance
- **Power consumption analysis** for energy-efficient deployments
- **Production-ready prediction system** with robust statistical validation
- **Vendor-agnostic optimization** across CPU, GPU, and accelerator platforms

## Project Structure
```
ai-inference-prediction/
â”œâ”€â”€ README.md
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ text_generation/
â”‚   â”‚   â”œâ”€â”€ gpt2/
â”‚   â”‚   â”‚   â”œâ”€â”€ gpt2_test.py
â”‚   â”‚   â”‚   â”œâ”€â”€ gpt2-cpu.dstack.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ gpt2-gpu24.dstack.yml
â”‚   â”‚   â”‚   â”œâ”€â”€ gpt2-gpu80.dstack.yml
â”‚   â”‚   â”‚   â””â”€â”€ gpt2-amd.dstack.yml
â”‚   â”‚   â”œâ”€â”€ llama8b/
â”‚   â”‚   â”‚   â”œâ”€â”€ llama8b_test.py
â”‚   â”‚   â”‚   â””â”€â”€ llama8b-*.dstack.yml
â”‚   â”‚   â”œâ”€â”€ llama70b/
â”‚   â”‚   â”œâ”€â”€ deepseek/
â”‚   â”‚   â””â”€â”€ llama405b/
â”‚   â””â”€â”€ tabular_ml/
â”‚       â”œâ”€â”€ xgboost/
â”‚       â”‚   â”œâ”€â”€ xgboost_test.py
â”‚       â”‚   â””â”€â”€ xgboost-cpu.dstack.yml
â”‚       â””â”€â”€ sklearn/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ text_generation/
â”‚   â”‚   â”œâ”€â”€ gpt2/
â”‚   â”‚   â”œâ”€â”€ llama8b/
â”‚   â”‚   â”œâ”€â”€ llama70b/
â”‚   â”‚   â”œâ”€â”€ combined/
â”‚   â”‚   â”œâ”€â”€ prediction/
â”‚   â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”‚   â”œâ”€â”€ test/
â”‚   â”‚   â”‚   â””â”€â”€ performance/
â”‚   â”œâ”€â”€ tabular_ml/
â”‚   â”‚   â”œâ”€â”€ xgboost/
â”‚   â”‚   â”œâ”€â”€ combined/
â”‚   â”‚   â””â”€â”€ prediction/
â”‚   â”‚       â”œâ”€â”€ train/
â”‚   â”‚       â”œâ”€â”€ test/
â”‚   â”‚       â””â”€â”€ performance/
â”‚   â””â”€â”€ final_summary_datasets/
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ stream1_data_generation/
â”‚   â”‚   â”œâ”€â”€ run_experiments.py
â”‚   â”‚   â””â”€â”€ metrics_collection.py
â”‚   â”œâ”€â”€ stream2_dataset_creation/
â”‚   â”‚   â”œâ”€â”€ combine_datasets.py
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   â””â”€â”€ eda.py
â”‚   â”œâ”€â”€ stream3_model_training/
â”‚   â”‚   â”œâ”€â”€ train_models.py
â”‚   â”‚   â”œâ”€â”€ model_comparison.py
â”‚   â”‚   â””â”€â”€ ensemble.py
â”‚   â””â”€â”€ stream4_feature_analysis/
â”‚       â”œâ”€â”€ shap_analysis.py
â”‚       â”œâ”€â”€ feature_selection.py
â”‚       â””â”€â”€ insights.py
â””â”€â”€ utils/
    â””â”€â”€ shared_functions.py
```

## Setup dstack Locally

1. **Install dstack CLI:**
```bash
pip install dstack
```

2. **Sign up for dstack Sky:**
- Go to https://sky.dstack.ai
- Sign up with GitHub account

3. **Configure CLI:**
```bash
dstack project add \
    --name main \
    --url https://sky.dstack.ai \
    --token <your-token-from-sky-dashboard>
```

4. **Initialize project:**
```bash
cd ai-inference-prediction
dstack init
```

## 4-Stream Workflow

1. **Data Generation** - Run model tests, collect metrics via dstack
2. **Dataset Creation** - Feature engineering, EDA on parquet files
3. **Model Training** - Train prediction models, compare architectures  
4. **Feature Analysis** - SHAP analysis, feature selection

## Testing Priority
**Models (smallest â†’ largest):**
1. GPT-2 small (124M) â†’ medium (355M) â†’ Llama 8B â†’ XGBoost â†’ Llama 70B â†’ DeepSeek-R1 â†’ Llama 405B

**Infrastructure (fastest startup time â†’ slowest):**
1. CPU-only â†’ Single GPU (24GB) â†’ Single GPU (80GB) â†’ AMD MI300X â†’ Multi-GPU â†’ TPU

## Tech Stack
- **Infrastructure**: dstack Sky
- **Data**: Parquet files via Google Drive
- **Serving**: vLLM, TGI, SGLang
- **Analysis**: pandas, SHAP, scikit-learn

## Quick Start
1. Complete dstack setup above
2. Run first test: 
```bash
dstack apply -f models/text_generation/gpt2/gpt2-cpu.dstack.yml
```
3. Download results to data folder:
```bash
dstack logs gpt2-cpu-test --download ./data/text_generation/gpt2/
```
4. Upload parquet files to Google Drive manually